# -*- coding: utf-8 -*-
"""
This file implements the notion of a Long Short Term Memory (LSTM) network. For
more information on LSTM's and the pytorch implementation see:
https://pytorch.org/docs/master/generated/torch.nn.LSTM.html

Future improvements:
This model currently trains on 17 videos and annotates one. If one were to use
this on a company level, it would need a lot more data. Another way to improve
the results of the model is to train the model specifically on one video
creator or train the model on one genre of videos. Especially when a video
creator uses the same intro or has certain catchphrases this can be useful. We
therefore recommend creating a specialised dataset in adjusting the contents of
the 'directory' variable.
"""
import os
from typing import List, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import srt

from . import asr
from . import caption

# Type aliases
Caption = List[Union[asr.Word, asr.Punc]]
Groups = List[Caption]

def prepare_sequence(seq, to_ix):
    """
    Returns a map of the words to the indices as a tensor.
    """
    idxs = [to_ix.get(w, 0) for w in seq]
    return torch.tensor(idxs, dtype=torch.long)


def create_traindata(directory: str) -> List[List[str]]:
    """
    Preprocesses the training data.

    Adds the eoc tags at the end of each caption and replaces the newlines
    with the nl tag. Preprocesses the data in a way that punctuationsymbols
    can be learned by the model.

    Args:
        directory: The directory containing the trainingset.

    Returns:
        A list of a captions with added tags and punctuation.
    """
    training_data = []
    for entry in os.scandir(directory):
        filename = entry.path
        with open(filename) as subs:
            training_data = []

            for sub in list(srt.parse(subs)):
                # Add the eoc tags at the end of each caption
                sub.content += " <eoc>"

                # Preprocess the sub to indepently train on punctuation and nl's
                sub.content = sub.content.replace('\n', ' <nl> ')
                sub.content = sub.content.replace('.', ' .')
                sub.content = sub.content.replace('?', ' ?')
                sub.content = sub.content.replace('!', ' !')
                sub.content = sub.content.replace(',', ' ,')
                sub.content = sub.content.replace(';', ' ;')
                sub.content = sub.content.replace(':', ' :')
                training_data.append(sub.content.lower().split())

    return training_data


def create_testdata(training_data: List[List[str]], groups: Groups) -> List[List[str]]:
    """
    Preprocess the testdata.

    Divide the transcript into sentences of input length, in a way that the
    LSTM can model it.

    Args:
        training_data: The trainingdata, generated by the create_traindata
            function
        groups: Groups in our custom Caption-list dataformat.
            Formed by asr.ASR().groups()

    Returns:
        The evaluationdata, formed into groups of the designated input format.
    """
    max_len = max([len(x) for x in training_data])

    test_set = [word.text for word in groups]
    eval_data = []

    # Divide the transcript into sentences of input length
    for i in range(len(test_set) // max_len):
        eval_data.append(test_set[i*max_len:i*max_len+max_len])


    eval_data.append(test_set[(i+1)*max_len:
                              (i+1)*max_len+len(test_set)%max_len])

    return eval_data

# The dimensions of the LSTM.
EMBEDDING_DIM = 64
HIDDEN_DIM = 64


class LSTMCaption(nn.Module):

    def __init__(self, embedding_dim, hidden_dim, vocab_size):
        super(LSTMCaption, self).__init__()
        self.hidden_dim = hidden_dim

        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)

        # The LSTM takes word embeddings as inputs, and outputs hidden states
        # with dimensionality hidden_dim.
        self.lstm = nn.LSTM(embedding_dim, hidden_dim)

        # The linear layer that maps from hidden state space to vocabulary space
        self.hidden2output = nn.Linear(hidden_dim, vocab_size)

    def forward(self, sentence):
        embeds = self.word_embeddings(sentence)
        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))
        output_space = self.hidden2output(lstm_out.view(len(sentence), -1))
        output_scores = F.log_softmax(output_space, dim=1)
        return output_scores


def train(n_epochs: int, training_data: List[List[str]]):
    """Train the model on the trainingdata.

    Args:
        n_epochs: The number times the model goes over the entire trainingset.
        training_data: The trainingdata, generated by the create_traindata()
            function.
    """
    for _ in range(n_epochs):

        # Go once over the entire train data
        for sentence in training_data:
            # Step 1. Remember that Pytorch accumulates gradients.
            # We need to clear them out before each instance
            model.zero_grad()

            # Step 2. Get our inputs ready for the network, that is, turn them into
            # Tensors of word indices.
            sentence_in = prepare_sequence(sentence, word_to_ix)
            # targets = prepare_sequence(tags, tag_to_ix)

            # Step 3. Run our forward pass.
            output_scores = model(sentence_in)

            pad_idx = len(sentence)

            # Step 4. Compute the loss, gradients, and update the parameters by
            #  calling optimizer.step()
            loss = loss_function(output_scores[:pad_idx, :], sentence_in[:pad_idx])
            loss.backward()
            optimizer.step()

if __name__ == '__main__':
    # The directory containing the trainingset
    directory = r'../dataset/srt/'
    training_data = create_traindata(directory)

    # The path to the to be captioned file
    path = '../asr/sample01.asrOutput.json'

    groups = asr.ASR(path).groups()
    eval_data = create_testdata(training_data, groups)

    # Make an index set of the observed words
    word_to_ix = {'unk': 0}
    for sent in training_data:
        for word in sent:
            if word not in word_to_ix:
                word_to_ix[word] = len(word_to_ix)

    # Make a mapping from the index set to the words
    ix_to_word = {v:k for (k, v) in word_to_ix.items()}

    model = LSTMCaption(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix))
    loss_function = nn.NLLLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.1)

    train(5, training_data)

    # Caption the file
    with torch.no_grad():
        ix = 0
        optimized_groups = []
        caption_group = []

        for sentence in eval_data:
            inputs = prepare_sequence(sentence, word_to_ix)
            output_scores = model(inputs)

            indices = torch.argmax(output_scores, axis=1)
            modeled = [ix_to_word[index.item()] for index in indices]

            for model_word, word in zip(modeled, groups[ix:]):
                if model_word == '<eoc>':
                    optimized_groups.append(caption_group)
                    caption_group = []

                else:
                    caption_group.append(word)

                ix += 1

        optimized_groups.append(caption_group)

        # Write a file
        caption.write(optimized_groups, 'lstm_output.srt')
