# -*- coding: utf-8 -*-
"""Copy of caption_segmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kLPdn0or1eiPmhnF7Am8hX2xk6ELz1Jp
"""
import os

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import srt

import matplotlib.pyplot as plt
import asr

# map de woorden naar indices
def prepare_sequence(seq, to_ix):
    idxs = [to_ix.get(w, 0) for w in seq]
    return torch.tensor(idxs, dtype=torch.long)


# Preprocessing:
# Hier voegen we de tag <eoc> (End-Of-Caption) toe aan het einde van de caption groepen
# (+ meer preprocessing steps ..)

# Verder: het model neemt input van 6 woorden per keer, dus voor kortere zinnen
# vullen we met <pad> aan tot 6


def create_data(filename):
    with open(filename) as f:
        training_data = []
        for sub in list(srt.parse(f)):
            sub.content += " <eoc>"
            sub.content = sub.content.replace('\n', ' <nl> ')
            training_data.append(sub.content.lower().split())

    return training_data

training_data = create_data('../manual_subtitles/manual_2.srt')

eval_data = asr.ASR('../asr/sample01.asrOutput.json').transcript()

eval_data += " <eoc>"
eval_data = eval_data.replace('\n', ' <nl> ')
eval_data = eval_data.lower().split()

eval_data = [eval_data]

# Maak een index set van de geobserveerde woorden
word_to_ix = {'unk': 0}
for sent in training_data:
    for word in sent:
        if word not in word_to_ix:
            word_to_ix[word] = len(word_to_ix)
print(word_to_ix)

# maak een map terug van de index set naar de woorden
ix_to_word = {v:k for (k, v) in word_to_ix.items()}

print(ix_to_word)

# These will usually be more like 32 or 64 dimensional.
# We will keep them small, so we can see how the weights change as we train.
EMBEDDING_DIM = 64
HIDDEN_DIM = 2600

class LSTMCaption(nn.Module):

    def __init__(self, embedding_dim, hidden_dim, vocab_size):
        super(LSTMCaption, self).__init__()
        self.hidden_dim = hidden_dim

        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)

        # The LSTM takes word embeddings as inputs, and outputs hidden states
        # with dimensionality hidden_dim.
        self.lstm = nn.LSTM(embedding_dim, hidden_dim)

        # The linear layer that maps from hidden state space to tag space

        # in dit geval terug naar de vocabulary space
        self.hidden2output = nn.Linear(hidden_dim, vocab_size)

    def forward(self, sentence):
        embeds = self.word_embeddings(sentence)
        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))
        output_space = self.hidden2output(lstm_out.view(len(sentence), -1))
        output_scores = F.log_softmax(output_space, dim=1)
        return output_scores

model = LSTMCaption(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix))
loss_function = nn.NLLLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)


def train(n_epochs, training_data, eval_data, debug=False):
    train_losses = []
    val_losses = []

    for epoch in range(n_epochs):  # again, normally you would NOT do 300 epochs, it is toy data

        # Go once over the entire train data
        for sentence in training_data:
            # Step 1. Remember that Pytorch accumulates gradients.
            # We need to clear them out before each instance
            model.zero_grad()

            # Step 2. Get our inputs ready for the network, that is, turn them into
            # Tensors of word indices.
            sentence_in = prepare_sequence(sentence, word_to_ix)
            # targets = prepare_sequence(tags, tag_to_ix)

            # Step 3. Run our forward pass.
            output_scores = model(sentence_in)

            # Hier kunnen we even kijken hoe het gaat: dit laat de predictions van het model zien
            if debug:
                indices = torch.argmax(output_scores, axis=1)
                print([ix_to_word[index.item()] for index in indices])



            # Voor het evalueren van de loss willen we alleen de woorden meenemen, niet de padding
            try:
                pad_idx = sentence.index('<pad>')
            except:
                pad_idx = len(sentence)

            # Step 4. Compute the loss, gradients, and update the parameters by
            #  calling optimizer.step()
            loss = loss_function(output_scores[:pad_idx, :], sentence_in[:pad_idx])
            loss.backward()
            train_losses.append(loss.item())
            optimizer.step()

        # Na elke train epoch gaan we op de val set kijken hoe goed het model werkt
        with torch.no_grad():
            for sentence in eval_data:
                inputs = prepare_sequence(sentence, word_to_ix)
                output_scores = model(inputs)


                # (even kijken naar wat het model predict)
                if debug:
                    indices = torch.argmax(output_scores, axis=1)
                    print([ix_to_word[index.item()] for index in indices])

                val_loss = loss_function(output_scores, sentence_in)
                val_losses.append(val_loss)

    # Validation curve plotten
    plt.plot(val_losses)
    plt.title("valiation loss")
    plt.show()


    # Train curve plotten
    plt.plot(train_losses)
    plt.title("training loss")
    plt.show()

train(2, training_data, eval_data, debug=True)

